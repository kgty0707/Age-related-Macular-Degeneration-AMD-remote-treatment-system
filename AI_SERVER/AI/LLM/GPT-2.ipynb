{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96edc966-e8dc-4d33-b453-0074e160f225",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f19037e-cc77-4f2a-9e02-0566bfac095d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2Config {\n",
      "  \"_name_or_path\": \"gpt2\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.39.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoConfig\n",
    "\n",
    "config = AutoConfig.from_pretrained('gpt2')\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "383fcbdd-63a3-4dc3-a7bb-003853b8ae32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting accelerate\n",
      "  Downloading accelerate-0.29.2-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\kgty\\anaconda3\\envs\\gpu\\lib\\site-packages (from accelerate) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\kgty\\anaconda3\\envs\\gpu\\lib\\site-packages (from accelerate) (23.2)\n",
      "Requirement already satisfied: psutil in c:\\users\\kgty\\anaconda3\\envs\\gpu\\lib\\site-packages (from accelerate) (5.9.0)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\kgty\\anaconda3\\envs\\gpu\\lib\\site-packages (from accelerate) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.10.0 in c:\\users\\kgty\\anaconda3\\envs\\gpu\\lib\\site-packages (from accelerate) (2.2.2)\n",
      "Requirement already satisfied: huggingface-hub in c:\\users\\kgty\\anaconda3\\envs\\gpu\\lib\\site-packages (from accelerate) (0.22.2)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\kgty\\anaconda3\\envs\\gpu\\lib\\site-packages (from accelerate) (0.4.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\kgty\\anaconda3\\envs\\gpu\\lib\\site-packages (from torch>=1.10.0->accelerate) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\kgty\\anaconda3\\envs\\gpu\\lib\\site-packages (from torch>=1.10.0->accelerate) (4.9.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\kgty\\anaconda3\\envs\\gpu\\lib\\site-packages (from torch>=1.10.0->accelerate) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\kgty\\anaconda3\\envs\\gpu\\lib\\site-packages (from torch>=1.10.0->accelerate) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\kgty\\anaconda3\\envs\\gpu\\lib\\site-packages (from torch>=1.10.0->accelerate) (3.1.3)\n",
      "Requirement already satisfied: fsspec in c:\\users\\kgty\\anaconda3\\envs\\gpu\\lib\\site-packages (from torch>=1.10.0->accelerate) (2024.2.0)\n",
      "Requirement already satisfied: requests in c:\\users\\kgty\\anaconda3\\envs\\gpu\\lib\\site-packages (from huggingface-hub->accelerate) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\kgty\\anaconda3\\envs\\gpu\\lib\\site-packages (from huggingface-hub->accelerate) (4.66.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\kgty\\anaconda3\\envs\\gpu\\lib\\site-packages (from tqdm>=4.42.1->huggingface-hub->accelerate) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\kgty\\anaconda3\\envs\\gpu\\lib\\site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\kgty\\anaconda3\\envs\\gpu\\lib\\site-packages (from requests->huggingface-hub->accelerate) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\kgty\\anaconda3\\envs\\gpu\\lib\\site-packages (from requests->huggingface-hub->accelerate) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\kgty\\anaconda3\\envs\\gpu\\lib\\site-packages (from requests->huggingface-hub->accelerate) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\kgty\\anaconda3\\envs\\gpu\\lib\\site-packages (from requests->huggingface-hub->accelerate) (2023.7.22)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\kgty\\anaconda3\\envs\\gpu\\lib\\site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
      "Downloading accelerate-0.29.2-py3-none-any.whl (297 kB)\n",
      "   ---------------------------------------- 0.0/297.4 kB ? eta -:--:--\n",
      "   --------------- ------------------------ 112.6/297.4 kB 3.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 297.4/297.4 kB 4.6 MB/s eta 0:00:00\n",
      "Installing collected packages: accelerate\n",
      "Successfully installed accelerate-0.29.2\n"
     ]
    }
   ],
   "source": [
    "!pip install accelerate -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e136771e-8b45-4660-a82e-e7e97d841e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gpt2_finetuned\", \n",
    "    num_train_epochs=3, \n",
    "    per_device_train_batch_size=16, \n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08530aef-d86a-4e47-a711-745f1a37eb4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, pipeline\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "180b2925-c46e-423b-9679-85876c8b7ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2 = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b7b6e21-b06f-4938-8297-cf565205d294",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** User Info ** ['1', 'he is a student.'], ['2', 'he is a computer engeenering student.'] ** end User Info ** Q. How can I make money using AI tools? A. Don't ask me like that. I only make money using a tool in general. Q. Who uses tools? A. You don't need them. These are some of the people who have been using Google algorithms. A. I want to make this money. The money I make is based on what I learn. Q. What is your investment in the project? A. I have been doing something before from the early days. What are my plans now? A. I am starting from scratch with an initial $1K investment and will spend as much time doing as I'm willing to invest. Q. What resources/gatherings do you have? A. I love to learn new things and try new projects. I am very interested in the various\n"
     ]
    }
   ],
   "source": [
    "generated_texts = gpt2(\"** User Info ** ['1', 'he is a student.'], ['2', 'he is a computer engeenering student.'] ** end User Info ** Q. How can I make money using AI tools? A.\", max_length=200, truncation=True)\n",
    "print(generated_texts[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379560ed-938b-4938-8eef-16f3fa20a019",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "76040f00-dd40-4973-8f91-1f3e91b66bd4",
   "metadata": {},
   "source": [
    "# GPT-2 Fine-Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f4e8a2-821f-4d36-a876-6e4473033853",
   "metadata": {},
   "source": [
    "### 데이터셋 구축"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e9ed0240-c406-4cbc-a038-16ed52f0fc8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "file_name = \"./data/020.Text by Topic Casual Conversation Data.zip\"\n",
    "output_dir = \"./data/casual_conversation_data\"\n",
    "format = \"zip\"\n",
    "shutil.unpack_archive(file_name, output_dir, format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ffc0b841-cf39-4a2d-8da3-8e17acca1386",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pytorch-lightning\n",
      "  Downloading pytorch_lightning-2.2.2-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: numpy>=1.17.2 in c:\\users\\kgty\\anaconda3\\envs\\gpu\\lib\\site-packages (from pytorch-lightning) (1.26.4)\n",
      "Requirement already satisfied: torch>=1.13.0 in c:\\users\\kgty\\anaconda3\\envs\\gpu\\lib\\site-packages (from pytorch-lightning) (2.2.2)\n",
      "Requirement already satisfied: tqdm>=4.57.0 in c:\\users\\kgty\\anaconda3\\envs\\gpu\\lib\\site-packages (from pytorch-lightning) (4.66.2)\n",
      "Requirement already satisfied: PyYAML>=5.4 in c:\\users\\kgty\\anaconda3\\envs\\gpu\\lib\\site-packages (from pytorch-lightning) (6.0.1)\n",
      "Requirement already satisfied: fsspec>=2022.5.0 in c:\\users\\kgty\\anaconda3\\envs\\gpu\\lib\\site-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (2024.2.0)\n",
      "Collecting torchmetrics>=0.7.0 (from pytorch-lightning)\n",
      "  Downloading torchmetrics-1.3.2-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\kgty\\anaconda3\\envs\\gpu\\lib\\site-packages (from pytorch-lightning) (23.2)\n",
      "Requirement already satisfied: typing-extensions>=4.4.0 in c:\\users\\kgty\\anaconda3\\envs\\gpu\\lib\\site-packages (from pytorch-lightning) (4.9.0)\n",
      "Collecting lightning-utilities>=0.8.0 (from pytorch-lightning)\n",
      "  Downloading lightning_utilities-0.11.2-py3-none-any.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\users\\kgty\\anaconda3\\envs\\gpu\\lib\\site-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (3.9.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\kgty\\anaconda3\\envs\\gpu\\lib\\site-packages (from lightning-utilities>=0.8.0->pytorch-lightning) (68.2.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\kgty\\anaconda3\\envs\\gpu\\lib\\site-packages (from torch>=1.13.0->pytorch-lightning) (3.13.1)\n",
      "Requirement already satisfied: sympy in c:\\users\\kgty\\anaconda3\\envs\\gpu\\lib\\site-packages (from torch>=1.13.0->pytorch-lightning) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\kgty\\anaconda3\\envs\\gpu\\lib\\site-packages (from torch>=1.13.0->pytorch-lightning) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\kgty\\anaconda3\\envs\\gpu\\lib\\site-packages (from torch>=1.13.0->pytorch-lightning) (3.1.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\kgty\\anaconda3\\envs\\gpu\\lib\\site-packages (from tqdm>=4.57.0->pytorch-lightning) (0.4.6)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\kgty\\anaconda3\\envs\\gpu\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\kgty\\anaconda3\\envs\\gpu\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\kgty\\anaconda3\\envs\\gpu\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\kgty\\anaconda3\\envs\\gpu\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\kgty\\anaconda3\\envs\\gpu\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in c:\\users\\kgty\\anaconda3\\envs\\gpu\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (4.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\kgty\\anaconda3\\envs\\gpu\\lib\\site-packages (from jinja2->torch>=1.13.0->pytorch-lightning) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\kgty\\anaconda3\\envs\\gpu\\lib\\site-packages (from sympy->torch>=1.13.0->pytorch-lightning) (1.3.0)\n",
      "Requirement already satisfied: idna>=2.0 in c:\\users\\kgty\\anaconda3\\envs\\gpu\\lib\\site-packages (from yarl<2.0,>=1.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (2.10)\n",
      "Downloading pytorch_lightning-2.2.2-py3-none-any.whl (801 kB)\n",
      "   ---------------------------------------- 0.0/801.9 kB ? eta -:--:--\n",
      "   ----------------- --------------------- 368.6/801.9 kB 11.6 MB/s eta 0:00:01\n",
      "   --------------------------------------- 801.9/801.9 kB 10.1 MB/s eta 0:00:00\n",
      "Downloading lightning_utilities-0.11.2-py3-none-any.whl (26 kB)\n",
      "Downloading torchmetrics-1.3.2-py3-none-any.whl (841 kB)\n",
      "   ---------------------------------------- 0.0/841.5 kB ? eta -:--:--\n",
      "   --------------------------------------  839.7/841.5 kB 25.9 MB/s eta 0:00:01\n",
      "   --------------------------------------- 841.5/841.5 kB 17.7 MB/s eta 0:00:00\n",
      "Installing collected packages: lightning-utilities, torchmetrics, pytorch-lightning\n",
      "Successfully installed lightning-utilities-0.11.2 pytorch-lightning-2.2.2 torchmetrics-1.3.2\n"
     ]
    }
   ],
   "source": [
    "!pip install pytorch-lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "490ed9c8-3dce-40d6-94a0-0d98718a88c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pytorch-forecasting\n",
      "  Downloading pytorch_forecasting-1.0.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting fastapi>=0.80 (from pytorch-forecasting)\n",
      "  Using cached fastapi-0.110.1-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting lightning<3.0.0,>=2.0.0 (from pytorch-forecasting)\n",
      "  Downloading lightning-2.2.2-py3-none-any.whl.metadata (53 kB)\n",
      "     ---------------------------------------- 0.0/53.4 kB ? eta -:--:--\n",
      "     ---------------------------------------- 53.4/53.4 kB 1.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: matplotlib in c:\\users\\kgty\\anaconda3\\envs\\gpu\\lib\\site-packages (from pytorch-forecasting) (3.8.4)\n",
      "Collecting optuna<4.0.0,>=3.1.0 (from pytorch-forecasting)\n",
      "  Downloading optuna-3.6.1-py3-none-any.whl.metadata (17 kB)\n",
      "Requirement already satisfied: pandas<=3.0.0,>=1.3.0 in c:\\users\\kgty\\anaconda3\\envs\\gpu\\lib\\site-packages (from pytorch-forecasting) (2.2.1)\n",
      "Collecting pytorch-optimizer<3.0.0,>=2.5.1 (from pytorch-forecasting)\n",
      "  Downloading pytorch_optimizer-2.12.0-py3-none-any.whl.metadata (46 kB)\n",
      "     ---------------------------------------- 0.0/46.1 kB ? eta -:--:--\n",
      "     ---------------------------------------- 46.1/46.1 kB ? eta 0:00:00\n",
      "Collecting scikit-learn<2.0,>=1.2 (from pytorch-forecasting)\n",
      "  Downloading scikit_learn-1.4.2-cp310-cp310-win_amd64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: scipy<2.0,>=1.8 in c:\\users\\kgty\\anaconda3\\envs\\gpu\\lib\\site-packages (from pytorch-forecasting) (1.13.0)\n",
      "Collecting statsmodels (from pytorch-forecasting)\n",
      "  Downloading statsmodels-0.14.2-cp310-cp310-win_amd64.whl.metadata (9.5 kB)\n",
      "Requirement already satisfied: torch<3.0.0,>=2.0.0 in c:\\users\\kgty\\anaconda3\\envs\\gpu\\lib\\site-packages (from pytorch-forecasting) (2.2.2)\n",
      "Collecting pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 (from fastapi>=0.80->pytorch-forecasting)\n",
      "  Downloading pydantic-2.7.0-py3-none-any.whl.metadata (103 kB)\n",
      "     ---------------------------------------- 0.0/103.4 kB ? eta -:--:--\n",
      "     -------------------------------------- 103.4/103.4 kB 5.8 MB/s eta 0:00:00\n",
      "Collecting starlette<0.38.0,>=0.37.2 (from fastapi>=0.80->pytorch-forecasting)\n",
      "  Using cached starlette-0.37.2-py3-none-any.whl.metadata (5.9 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\kgty\\anaconda3\\envs\\gpu\\lib\\site-packages (from fastapi>=0.80->pytorch-forecasting) (4.9.0)\n",
      "Requirement already satisfied: PyYAML<8.0,>=5.4 in c:\\users\\kgty\\anaconda3\\envs\\gpu\\lib\\site-packages (from lightning<3.0.0,>=2.0.0->pytorch-forecasting) (6.0.1)\n",
      "Requirement already satisfied: fsspec<2025.0,>=2022.5.0 in c:\\users\\kgty\\anaconda3\\envs\\gpu\\lib\\site-packages (from fsspec[http]<2025.0,>=2022.5.0->lightning<3.0.0,>=2.0.0->pytorch-forecasting) (2024.2.0)\n",
      "Requirement already satisfied: lightning-utilities<2.0,>=0.8.0 in c:\\users\\kgty\\anaconda3\\envs\\gpu\\lib\\site-packages (from lightning<3.0.0,>=2.0.0->pytorch-forecasting) (0.11.2)\n",
      "Requirement already satisfied: numpy<3.0,>=1.17.2 in c:\\users\\kgty\\anaconda3\\envs\\gpu\\lib\\site-packages (from lightning<3.0.0,>=2.0.0->pytorch-forecasting) (1.26.4)\n",
      "Requirement already satisfied: packaging<25.0,>=20.0 in c:\\users\\kgty\\anaconda3\\envs\\gpu\\lib\\site-packages (from lightning<3.0.0,>=2.0.0->pytorch-forecasting) (23.2)\n",
      "Requirement already satisfied: torchmetrics<3.0,>=0.7.0 in c:\\users\\kgty\\anaconda3\\envs\\gpu\\lib\\site-packages (from lightning<3.0.0,>=2.0.0->pytorch-forecasting) (1.3.2)\n",
      "Requirement already satisfied: tqdm<6.0,>=4.57.0 in c:\\users\\kgty\\anaconda3\\envs\\gpu\\lib\\site-packages (from lightning<3.0.0,>=2.0.0->pytorch-forecasting) (4.66.2)\n",
      "Requirement already satisfied: pytorch-lightning in c:\\users\\kgty\\anaconda3\\envs\\gpu\\lib\\site-packages (from lightning<3.0.0,>=2.0.0->pytorch-forecasting) (2.2.2)\n",
      "Collecting alembic>=1.5.0 (from optuna<4.0.0,>=3.1.0->pytorch-forecasting)\n",
      "  Downloading alembic-1.13.1-py3-none-any.whl.metadata (7.4 kB)\n",
      "Collecting colorlog (from optuna<4.0.0,>=3.1.0->pytorch-forecasting)\n",
      "  Downloading colorlog-6.8.2-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting sqlalchemy>=1.3.0 (from optuna<4.0.0,>=3.1.0->pytorch-forecasting)\n",
      "  Downloading SQLAlchemy-2.0.29-cp310-cp310-win_amd64.whl.metadata (9.8 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\kgty\\anaconda3\\envs\\gpu\\lib\\site-packages (from pandas<=3.0.0,>=1.3.0->pytorch-forecasting) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\kgty\\anaconda3\\envs\\gpu\\lib\\site-packages (from pandas<=3.0.0,>=1.3.0->pytorch-forecasting) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\kgty\\anaconda3\\envs\\gpu\\lib\\site-packages (from pandas<=3.0.0,>=1.3.0->pytorch-forecasting) (2024.1)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn<2.0,>=1.2->pytorch-forecasting)\n",
      "  Downloading joblib-1.4.0-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=2.0.0 (from scikit-learn<2.0,>=1.2->pytorch-forecasting)\n",
      "  Using cached threadpoolctl-3.4.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\kgty\\anaconda3\\envs\\gpu\\lib\\site-packages (from torch<3.0.0,>=2.0.0->pytorch-forecasting) (3.13.1)\n",
      "Requirement already satisfied: sympy in c:\\users\\kgty\\anaconda3\\envs\\gpu\\lib\\site-packages (from torch<3.0.0,>=2.0.0->pytorch-forecasting) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\kgty\\anaconda3\\envs\\gpu\\lib\\site-packages (from torch<3.0.0,>=2.0.0->pytorch-forecasting) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\kgty\\anaconda3\\envs\\gpu\\lib\\site-packages (from torch<3.0.0,>=2.0.0->pytorch-forecasting) (3.1.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\kgty\\anaconda3\\envs\\gpu\\lib\\site-packages (from matplotlib->pytorch-forecasting) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\kgty\\anaconda3\\envs\\gpu\\lib\\site-packages (from matplotlib->pytorch-forecasting) (0.10.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\kgty\\anaconda3\\envs\\gpu\\lib\\site-packages (from matplotlib->pytorch-forecasting) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\kgty\\anaconda3\\envs\\gpu\\lib\\site-packages (from matplotlib->pytorch-forecasting) (1.4.5)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\kgty\\anaconda3\\envs\\gpu\\lib\\site-packages (from matplotlib->pytorch-forecasting) (10.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\kgty\\anaconda3\\envs\\gpu\\lib\\site-packages (from matplotlib->pytorch-forecasting) (3.1.2)\n",
      "Collecting patsy>=0.5.6 (from statsmodels->pytorch-forecasting)\n",
      "  Downloading patsy-0.5.6-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting Mako (from alembic>=1.5.0->optuna<4.0.0,>=3.1.0->pytorch-forecasting)\n",
      "  Downloading Mako-1.3.3-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: six in c:\\users\\kgty\\anaconda3\\envs\\gpu\\lib\\site-packages (from cycler>=0.10->matplotlib->pytorch-forecasting) (1.16.0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\users\\kgty\\anaconda3\\envs\\gpu\\lib\\site-packages (from fsspec[http]<2025.0,>=2022.5.0->lightning<3.0.0,>=2.0.0->pytorch-forecasting) (3.9.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\kgty\\anaconda3\\envs\\gpu\\lib\\site-packages (from lightning-utilities<2.0,>=0.8.0->lightning<3.0.0,>=2.0.0->pytorch-forecasting) (68.2.2)\n",
      "Collecting annotated-types>=0.4.0 (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi>=0.80->pytorch-forecasting)\n",
      "  Using cached annotated_types-0.6.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting pydantic-core==2.18.1 (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi>=0.80->pytorch-forecasting)\n",
      "  Downloading pydantic_core-2.18.1-cp310-none-win_amd64.whl.metadata (6.7 kB)\n",
      "Collecting greenlet!=0.4.17 (from sqlalchemy>=1.3.0->optuna<4.0.0,>=3.1.0->pytorch-forecasting)\n",
      "  Downloading greenlet-3.0.3-cp310-cp310-win_amd64.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.4.0 in c:\\users\\kgty\\anaconda3\\envs\\gpu\\lib\\site-packages (from starlette<0.38.0,>=0.37.2->fastapi>=0.80->pytorch-forecasting) (4.2.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\kgty\\anaconda3\\envs\\gpu\\lib\\site-packages (from tqdm<6.0,>=4.57.0->lightning<3.0.0,>=2.0.0->pytorch-forecasting) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\kgty\\anaconda3\\envs\\gpu\\lib\\site-packages (from jinja2->torch<3.0.0,>=2.0.0->pytorch-forecasting) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\kgty\\anaconda3\\envs\\gpu\\lib\\site-packages (from sympy->torch<3.0.0,>=2.0.0->pytorch-forecasting) (1.3.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\kgty\\anaconda3\\envs\\gpu\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning<3.0.0,>=2.0.0->pytorch-forecasting) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\kgty\\anaconda3\\envs\\gpu\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning<3.0.0,>=2.0.0->pytorch-forecasting) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\kgty\\anaconda3\\envs\\gpu\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning<3.0.0,>=2.0.0->pytorch-forecasting) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\kgty\\anaconda3\\envs\\gpu\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning<3.0.0,>=2.0.0->pytorch-forecasting) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\kgty\\anaconda3\\envs\\gpu\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning<3.0.0,>=2.0.0->pytorch-forecasting) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in c:\\users\\kgty\\anaconda3\\envs\\gpu\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning<3.0.0,>=2.0.0->pytorch-forecasting) (4.0.3)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\kgty\\anaconda3\\envs\\gpu\\lib\\site-packages (from anyio<5,>=3.4.0->starlette<0.38.0,>=0.37.2->fastapi>=0.80->pytorch-forecasting) (2.10)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\kgty\\anaconda3\\envs\\gpu\\lib\\site-packages (from anyio<5,>=3.4.0->starlette<0.38.0,>=0.37.2->fastapi>=0.80->pytorch-forecasting) (1.3.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\kgty\\anaconda3\\envs\\gpu\\lib\\site-packages (from anyio<5,>=3.4.0->starlette<0.38.0,>=0.37.2->fastapi>=0.80->pytorch-forecasting) (1.2.0)\n",
      "Downloading pytorch_forecasting-1.0.0-py3-none-any.whl (140 kB)\n",
      "   ---------------------------------------- 0.0/140.4 kB ? eta -:--:--\n",
      "   ---------------------------------------- 140.4/140.4 kB 4.2 MB/s eta 0:00:00\n",
      "Using cached fastapi-0.110.1-py3-none-any.whl (91 kB)\n",
      "Downloading lightning-2.2.2-py3-none-any.whl (2.0 MB)\n",
      "   ---------------------------------------- 0.0/2.0 MB ? eta -:--:--\n",
      "   -------------- ------------------------- 0.7/2.0 MB 14.6 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 1.6/2.0 MB 16.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.0/2.0 MB 18.1 MB/s eta 0:00:00\n",
      "Downloading optuna-3.6.1-py3-none-any.whl (380 kB)\n",
      "   ---------------------------------------- 0.0/380.1 kB ? eta -:--:--\n",
      "   --------------------------------------- 380.1/380.1 kB 24.7 MB/s eta 0:00:00\n",
      "Downloading pytorch_optimizer-2.12.0-py3-none-any.whl (155 kB)\n",
      "   ---------------------------------------- 0.0/155.8 kB ? eta -:--:--\n",
      "   ---------------------------------------- 155.8/155.8 kB ? eta 0:00:00\n",
      "Downloading scikit_learn-1.4.2-cp310-cp310-win_amd64.whl (10.6 MB)\n",
      "   ---------------------------------------- 0.0/10.6 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 1.7/10.6 MB 36.5 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 4.1/10.6 MB 43.8 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 6.3/10.6 MB 50.2 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 6.3/10.6 MB 50.2 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 6.3/10.6 MB 50.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  10.5/10.6 MB 38.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 10.6/10.6 MB 36.4 MB/s eta 0:00:00\n",
      "Downloading statsmodels-0.14.2-cp310-cp310-win_amd64.whl (9.8 MB)\n",
      "   ---------------------------------------- 0.0/9.8 MB ? eta -:--:--\n",
      "   ------------- -------------------------- 3.4/9.8 MB 106.6 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 7.3/9.8 MB 92.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 9.8/9.8 MB 78.7 MB/s eta 0:00:00\n",
      "Downloading alembic-1.13.1-py3-none-any.whl (233 kB)\n",
      "   ---------------------------------------- 0.0/233.4 kB ? eta -:--:--\n",
      "   --------------------------------------- 233.4/233.4 kB 13.9 MB/s eta 0:00:00\n",
      "Downloading joblib-1.4.0-py3-none-any.whl (301 kB)\n",
      "   ---------------------------------------- 0.0/301.2 kB ? eta -:--:--\n",
      "   --------------------------------------- 301.2/301.2 kB 18.2 MB/s eta 0:00:00\n",
      "Downloading patsy-0.5.6-py2.py3-none-any.whl (233 kB)\n",
      "   ---------------------------------------- 0.0/233.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 233.9/233.9 kB ? eta 0:00:00\n",
      "Downloading pydantic-2.7.0-py3-none-any.whl (407 kB)\n",
      "   ---------------------------------------- 0.0/407.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 407.9/407.9 kB ? eta 0:00:00\n",
      "Downloading pydantic_core-2.18.1-cp310-none-win_amd64.whl (1.9 MB)\n",
      "   ---------------------------------------- 0.0/1.9 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.9/1.9 MB 61.5 MB/s eta 0:00:00\n",
      "Downloading SQLAlchemy-2.0.29-cp310-cp310-win_amd64.whl (2.1 MB)\n",
      "   ---------------------------------------- 0.0/2.1 MB ? eta -:--:--\n",
      "   ---------------------------------------  2.1/2.1 MB 64.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.1/2.1 MB 43.9 MB/s eta 0:00:00\n",
      "Using cached starlette-0.37.2-py3-none-any.whl (71 kB)\n",
      "Using cached threadpoolctl-3.4.0-py3-none-any.whl (17 kB)\n",
      "Downloading colorlog-6.8.2-py3-none-any.whl (11 kB)\n",
      "Using cached annotated_types-0.6.0-py3-none-any.whl (12 kB)\n",
      "Downloading greenlet-3.0.3-cp310-cp310-win_amd64.whl (292 kB)\n",
      "   ---------------------------------------- 0.0/292.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 292.3/292.3 kB ? eta 0:00:00\n",
      "Downloading Mako-1.3.3-py3-none-any.whl (78 kB)\n",
      "   ---------------------------------------- 0.0/78.8 kB ? eta -:--:--\n",
      "   ---------------------------------------- 78.8/78.8 kB ? eta 0:00:00\n",
      "Installing collected packages: threadpoolctl, pydantic-core, patsy, Mako, joblib, greenlet, colorlog, annotated-types, starlette, sqlalchemy, scikit-learn, pydantic, statsmodels, pytorch-optimizer, fastapi, alembic, optuna, lightning, pytorch-forecasting\n",
      "Successfully installed Mako-1.3.3 alembic-1.13.1 annotated-types-0.6.0 colorlog-6.8.2 fastapi-0.110.1 greenlet-3.0.3 joblib-1.4.0 lightning-2.2.2 optuna-3.6.1 patsy-0.5.6 pydantic-2.7.0 pydantic-core-2.18.1 pytorch-forecasting-1.0.0 pytorch-optimizer-2.12.0 scikit-learn-1.4.2 sqlalchemy-2.0.29 starlette-0.37.2 statsmodels-0.14.2 threadpoolctl-3.4.0\n"
     ]
    }
   ],
   "source": [
    "!pip install pytorch-forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f840e9-bda0-46e5-b436-6d3459515717",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9368563-d7f7-4a50-998e-51d6324105fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f9b409-32fd-4aeb-9f52-b357edeac96b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84050be-90cd-4134-ad4a-11a22a4d9719",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dab5f40a-8867-45c2-af9c-5a165307f032",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import re\n",
    "import torch\n",
    "import urllib.request\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from transformers.optimization import AdamW, get_cosine_schedule_with_warmup\n",
    "from transformers import PreTrainedTokenizerFast, GPT2LMHeadModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab0e0317-8089-4703-9a18-2a5e4964e10e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Q</th>\n",
       "      <th>A</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12시 땡!</td>\n",
       "      <td>하루가 또 가네요.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1지망 학교 떨어졌어</td>\n",
       "      <td>위로해 드립니다.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3박4일 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3박4일 정도 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PPL 심하네</td>\n",
       "      <td>눈살이 찌푸려지죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Q            A  label\n",
       "0           12시 땡!   하루가 또 가네요.      0\n",
       "1      1지망 학교 떨어졌어    위로해 드립니다.      0\n",
       "2     3박4일 놀러가고 싶다  여행은 언제나 좋죠.      0\n",
       "3  3박4일 정도 놀러가고 싶다  여행은 언제나 좋죠.      0\n",
       "4          PPL 심하네   눈살이 찌푸려지죠.      0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import urllib.request\n",
    "\n",
    "urllib.request.urlretrieve(\n",
    "    \"https://raw.githubusercontent.com/songys/Chatbot_data/master/ChatbotData.csv\",\n",
    "    filename=\"ChatBotData.csv\",\n",
    ")\n",
    "Chatbot_Data = pd.read_csv(\"ChatBotData.csv\")\n",
    "\n",
    "# Test 용으로 300개 데이터만 처리한다.\n",
    "Chatbot_Data = Chatbot_Data[:300]\n",
    "Chatbot_Data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f351212-8c37-4213-8585-314600be2601",
   "metadata": {},
   "source": [
    "## Tokenizer 기능\n",
    "\n",
    "1. Tokenizing : 입력 문자열을 token id로 변환(encoding), token id를 다시 문자열로 변환(decoding)의 기능\r",
    "2. \n",
    "기존의 구조(BPE, Sentencepiece 등)에 독립적으로 추가적인 token들을 추가하는 기능3. \r\n",
    "Special token들을 (mask, BOS, EOS 등) 관리하는 기능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "28f2bd03-1f6a-45f1-98b7-71f6fe8a5f73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'GPT2Tokenizer'. \n",
      "The class this function is called from is 'PreTrainedTokenizerFast'.\n"
     ]
    }
   ],
   "source": [
    "BOS = \"</s>\"\n",
    "EOS = \"</s>\"\n",
    "PAD = \"<pad>\"\n",
    "MASK = \"<unused0>\"\n",
    "Q_TKN = \"<usr>\"\n",
    "SENT = '<unused1>'\n",
    "A_TKN = \"<sys>\"\n",
    "\n",
    "\n",
    "# 허깅페이스 transformers 에 등록된 사전 학습된 koGTP2 토크나이저를 가져온다.\n",
    "koGPT2_TOKENIZER = PreTrainedTokenizerFast.from_pretrained(\"skt/kogpt2-base-v2\", bos_token=BOS, eos_token=EOS, unk_token=\"<unk>\", pad_token=PAD, mask_token=MASK,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "25cbe96f-ea5a-4750-bbdd-d74e67f626e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 챗봇 데이터를 처리하는 클래스를 만든다.\n",
    "class ChatbotDataset(Dataset):\n",
    "    def __init__(self, chats, max_len=40):  # 데이터셋의 전처리를 해주는 부분\n",
    "        self._data = chats\n",
    "        self.max_len = max_len\n",
    "        self.q_token = Q_TKN\n",
    "        self.a_token = A_TKN\n",
    "        self.sent_token = SENT\n",
    "        self.eos = EOS\n",
    "        self.mask = MASK\n",
    "        self.tokenizer = koGPT2_TOKENIZER\n",
    "\n",
    "    def __len__(self):  # chatbotdata 의 길이를 리턴한다.\n",
    "        return len(self._data)\n",
    "\n",
    "    def __getitem__(self, idx):  # 로드한 챗봇 데이터를 차례차례 DataLoader로 넘겨주는 메서드\n",
    "        turn = self._data.iloc[idx]\n",
    "        q = turn[\"Q\"]  # 질문을 가져온다.\n",
    "        q = re.sub(r\"([?.!,])\", r\" \", q)  # 구둣점들을 제거한다.\n",
    "\n",
    "        a = turn[\"A\"]  # 답변을 가져온다.\n",
    "        a = re.sub(r\"([?.!,])\", r\" \", a)  # 구둣점들을 제거한다.\n",
    "\n",
    "        q_toked = self.tokenizer.tokenize(self.q_token + q + self.sent_token)\n",
    "        q_len = len(q_toked)\n",
    "\n",
    "        a_toked = self.tokenizer.tokenize(self.a_token + a + self.eos)\n",
    "        a_len = len(a_toked)\n",
    "\n",
    "        #질문의 길이가 최대길이보다 크면\n",
    "        if q_len > self.max_len:\n",
    "            a_len = self.max_len - q_len        #답변의 길이를 최대길이 - 질문길이\n",
    "            if a_len <= 0:       #질문의 길이가 너무 길어 질문만으로 최대 길이를 초과 한다면\n",
    "                q_toked = q_toked[-(int(self.max_len / 2)) :]   #질문길이를 최대길이의 반으로 \n",
    "                q_len = len(q_toked)\n",
    "                a_len = self.max_len - q_len              #답변의 길이를 최대길이 - 질문길이\n",
    "            a_toked = a_toked[:a_len]\n",
    "            a_len = len(a_toked)\n",
    "\n",
    "        #질문의 길이 + 답변의 길이가 최대길이보다 크면\n",
    "        if q_len + a_len > self.max_len:\n",
    "            a_len = self.max_len - q_len        #답변의 길이를 최대길이 - 질문길이\n",
    "            if a_len <= 0:       #질문의 길이가 너무 길어 질문만으로 최대 길이를 초과 한다면\n",
    "                q_toked = q_toked[-(int(self.max_len / 2)) :]   #질문길이를 최대길이의 반으로 \n",
    "                q_len = len(q_toked)\n",
    "                a_len = self.max_len - q_len              #답변의 길이를 최대길이 - 질문길이\n",
    "            a_toked = a_toked[:a_len]\n",
    "            a_len = len(a_toked)\n",
    "\n",
    "        # 답변 labels = [mask, mask, ...., mask, ..., <bos>,..답변.. <eos>, <pad>....]\n",
    "        labels = [self.mask,] * q_len + a_toked[1:]\n",
    "\n",
    "        # mask = 질문길이 0 + 답변길이 1 + 나머지 0\n",
    "        mask = [0] * q_len + [1] * a_len + [0] * (self.max_len - q_len - a_len)\n",
    "        # 답변 labels을 index 로 만든다.\n",
    "        labels_ids = self.tokenizer.convert_tokens_to_ids(labels)\n",
    "        # 최대길이만큼 PADDING\n",
    "        while len(labels_ids) < self.max_len:\n",
    "            labels_ids += [self.tokenizer.pad_token_id]\n",
    "\n",
    "        # 질문 + 답변을 index 로 만든다.    \n",
    "        token_ids = self.tokenizer.convert_tokens_to_ids(q_toked + a_toked)\n",
    "        # 최대길이만큼 PADDING\n",
    "        while len(token_ids) < self.max_len:\n",
    "            token_ids += [self.tokenizer.pad_token_id]\n",
    "\n",
    "        #질문+답변, 마스크, 답변\n",
    "        return (token_ids, np.array(mask), labels_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e223ea1f-9138-4474-8c38-48b925b5ecd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_batch(batch):\n",
    "    data = [item[0] for item in batch]\n",
    "    mask = [item[1] for item in batch]\n",
    "    label = [item[2] for item in batch]\n",
    "    return torch.LongTensor(data), torch.LongTensor(mask), torch.LongTensor(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6b9a3822-f8b9-4b5a-ac2c-20fd60c1fa82",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "train_set = ChatbotDataset(Chatbot_Data, max_len=40)\n",
    "\n",
    "#윈도우 환경에서 num_workers 는 무조건 0으로 지정, 리눅스에서는 2\n",
    "train_dataloader = DataLoader(train_set, batch_size=32, num_workers=0, shuffle=True, collate_fn=collate_batch,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "57fbc22f-4ba5-4fea-bee7-965942ee2b80",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kgty\\AppData\\Local\\Temp\\ipykernel_26720\\2495599640.py:5: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\utils\\tensor_new.cpp:278.)\n",
      "  return torch.LongTensor(data), torch.LongTensor(mask), torch.LongTensor(label)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "token_ids ====>  tensor([[    2,  9244,  7584,  ...,     3,     3,     3],\n",
      "        [    2, 31279,  9341,  ...,     3,     3,     3],\n",
      "        [    2,  9546,  6969,  ...,     3,     3,     3],\n",
      "        ...,\n",
      "        [    2,  9020,  8263,  ...,     3,     3,     3],\n",
      "        [    2, 10715,  9511,  ...,     3,     3,     3],\n",
      "        [    2, 19855,  9350,  ...,     3,     3,     3]])\n",
      "mask =====>  tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]])\n",
      "label =====>  tensor([[9, 9, 9,  ..., 3, 3, 3],\n",
      "        [9, 9, 9,  ..., 3, 3, 3],\n",
      "        [9, 9, 9,  ..., 3, 3, 3],\n",
      "        ...,\n",
      "        [9, 9, 9,  ..., 3, 3, 3],\n",
      "        [9, 9, 9,  ..., 3, 3, 3],\n",
      "        [9, 9, 9,  ..., 3, 3, 3]])\n",
      "token_ids ====>  tensor([[    2, 20509,  7847,  ...,     3,     3,     3],\n",
      "        [    2, 11342,  9945,  ...,     3,     3,     3],\n",
      "        [    2, 10715,  9685,  ...,     3,     3,     3],\n",
      "        ...,\n",
      "        [    2, 10715, 18338,  ...,     3,     3,     3],\n",
      "        [    2,  9065,  7487,  ...,     3,     3,     3],\n",
      "        [    2,  9086,  6835,  ...,     3,     3,     3]])\n",
      "mask =====>  tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]])\n",
      "label =====>  tensor([[9, 9, 9,  ..., 3, 3, 3],\n",
      "        [9, 9, 9,  ..., 3, 3, 3],\n",
      "        [9, 9, 9,  ..., 3, 3, 3],\n",
      "        ...,\n",
      "        [9, 9, 9,  ..., 3, 3, 3],\n",
      "        [9, 9, 9,  ..., 3, 3, 3],\n",
      "        [9, 9, 9,  ..., 3, 3, 3]])\n",
      "token_ids ====>  tensor([[    2, 10715, 10628,  ...,     3,     3,     3],\n",
      "        [    2, 21435, 10973,  ...,     3,     3,     3],\n",
      "        [    2, 10464, 45847,  ...,     3,     3,     3],\n",
      "        ...,\n",
      "        [    2, 41671,  9070,  ...,     3,     3,     3],\n",
      "        [    2,  9085,  7597,  ...,     3,     3,     3],\n",
      "        [    2, 27820,  7380,  ...,     3,     3,     3]])\n",
      "mask =====>  tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]])\n",
      "label =====>  tensor([[9, 9, 9,  ..., 3, 3, 3],\n",
      "        [9, 9, 9,  ..., 3, 3, 3],\n",
      "        [9, 9, 9,  ..., 3, 3, 3],\n",
      "        ...,\n",
      "        [9, 9, 9,  ..., 3, 3, 3],\n",
      "        [9, 9, 9,  ..., 3, 3, 3],\n",
      "        [9, 9, 9,  ..., 3, 3, 3]])\n",
      "token_ids ====>  tensor([[    2, 11342, 16173,  ...,     3,     3,     3],\n",
      "        [    2, 10715, 11469,  ...,     3,     3,     3],\n",
      "        [    2,  9716, 22800,  ...,     3,     3,     3],\n",
      "        ...,\n",
      "        [    2, 25883, 11630,  ...,     3,     3,     3],\n",
      "        [    2, 17542, 49932,  ...,     3,     3,     3],\n",
      "        [    2, 10715, 11469,  ...,     3,     3,     3]])\n",
      "mask =====>  tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]])\n",
      "label =====>  tensor([[9, 9, 9,  ..., 3, 3, 3],\n",
      "        [9, 9, 9,  ..., 3, 3, 3],\n",
      "        [9, 9, 9,  ..., 3, 3, 3],\n",
      "        ...,\n",
      "        [9, 9, 9,  ..., 3, 3, 3],\n",
      "        [9, 9, 9,  ..., 3, 3, 3],\n",
      "        [9, 9, 9,  ..., 3, 3, 3]])\n",
      "token_ids ====>  tensor([[    2, 11018, 11732,  ...,     3,     3,     3],\n",
      "        [    2,  9815, 37655,  ...,     3,     3,     3],\n",
      "        [    2,  9244,  6958,  ...,     3,     3,     3],\n",
      "        ...,\n",
      "        [    2, 11204,  9049,  ...,     3,     3,     3],\n",
      "        [    2,  9815, 37655,  ...,     3,     3,     3],\n",
      "        [    2,  9779,  9301,  ...,     3,     3,     3]])\n",
      "mask =====>  tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]])\n",
      "label =====>  tensor([[9, 9, 9,  ..., 3, 3, 3],\n",
      "        [9, 9, 9,  ..., 3, 3, 3],\n",
      "        [9, 9, 9,  ..., 3, 3, 3],\n",
      "        ...,\n",
      "        [9, 9, 9,  ..., 3, 3, 3],\n",
      "        [9, 9, 9,  ..., 3, 3, 3],\n",
      "        [9, 9, 9,  ..., 3, 3, 3]])\n",
      "token_ids ====>  tensor([[    2, 23498,  7058,  ...,     3,     3,     3],\n",
      "        [    2, 19787, 49542,  ...,     3,     3,     3],\n",
      "        [    2,  9065, 36562,  ...,     3,     3,     3],\n",
      "        ...,\n",
      "        [    2, 31759, 10281,  ...,     3,     3,     3],\n",
      "        [    2, 10637,  8236,  ...,     3,     3,     3],\n",
      "        [    2,  9779, 13950,  ...,     3,     3,     3]])\n",
      "mask =====>  tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]])\n",
      "label =====>  tensor([[9, 9, 9,  ..., 3, 3, 3],\n",
      "        [9, 9, 9,  ..., 3, 3, 3],\n",
      "        [9, 9, 9,  ..., 3, 3, 3],\n",
      "        ...,\n",
      "        [9, 9, 9,  ..., 3, 3, 3],\n",
      "        [9, 9, 9,  ..., 3, 3, 3],\n",
      "        [9, 9, 9,  ..., 3, 3, 3]])\n",
      "token_ids ====>  tensor([[    2, 15983,  7673,  ...,     3,     3,     3],\n",
      "        [    2,  9085,  7597,  ...,     3,     3,     3],\n",
      "        [    2, 23356, 11732,  ...,     3,     3,     3],\n",
      "        ...,\n",
      "        [    2,  9099, 23282,  ...,     3,     3,     3],\n",
      "        [    2, 25883, 11630,  ...,     3,     3,     3],\n",
      "        [    2,  9716, 45555,  ...,     3,     3,     3]])\n",
      "mask =====>  tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]])\n",
      "label =====>  tensor([[9, 9, 9,  ..., 3, 3, 3],\n",
      "        [9, 9, 9,  ..., 3, 3, 3],\n",
      "        [9, 9, 9,  ..., 3, 3, 3],\n",
      "        ...,\n",
      "        [9, 9, 9,  ..., 3, 3, 3],\n",
      "        [9, 9, 9,  ..., 3, 3, 3],\n",
      "        [9, 9, 9,  ..., 3, 3, 3]])\n",
      "token_ids ====>  tensor([[    2, 13198,  9716,  ...,     3,     3,     3],\n",
      "        [    2, 13016,  6958,  ...,     3,     3,     3],\n",
      "        [    2, 19319, 46651,  ...,     3,     3,     3],\n",
      "        ...,\n",
      "        [    2, 22257, 22692,  ...,     3,     3,     3],\n",
      "        [    2,  9779, 10624,  ...,     3,     3,     3],\n",
      "        [    2,   739,  6910,  ...,     3,     3,     3]])\n",
      "mask =====>  tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]])\n",
      "label =====>  tensor([[9, 9, 9,  ..., 3, 3, 3],\n",
      "        [9, 9, 9,  ..., 3, 3, 3],\n",
      "        [9, 9, 9,  ..., 3, 3, 3],\n",
      "        ...,\n",
      "        [9, 9, 9,  ..., 3, 3, 3],\n",
      "        [9, 9, 9,  ..., 3, 3, 3],\n",
      "        [9, 9, 9,  ..., 3, 3, 3]])\n",
      "token_ids ====>  tensor([[    2,  9065,   394,  ...,     3,     3,     3],\n",
      "        [    2, 27137, 11732,  ...,     3,     3,     3],\n",
      "        [    2, 22311, 26089,  ...,     3,     3,     3],\n",
      "        ...,\n",
      "        [    2,  9067,  7653,  ...,     3,     3,     3],\n",
      "        [    2, 26629, 23799,  ...,     3,     3,     3],\n",
      "        [    2, 19787,  9183,  ...,     3,     3,     3]])\n",
      "mask =====>  tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]])\n",
      "label =====>  tensor([[9, 9, 9,  ..., 3, 3, 3],\n",
      "        [9, 9, 9,  ..., 3, 3, 3],\n",
      "        [9, 9, 9,  ..., 3, 3, 3],\n",
      "        ...,\n",
      "        [9, 9, 9,  ..., 3, 3, 3],\n",
      "        [9, 9, 9,  ..., 3, 3, 3],\n",
      "        [9, 9, 9,  ..., 3, 3, 3]])\n",
      "token_ids ====>  tensor([[    2,  9099, 13692, 14160,  9518, 18767, 49067,    10,     4,  9265,\n",
      "          7235, 12904, 25799,  8084,   739,     1,     3,     3,     3,     3,\n",
      "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3],\n",
      "        [    2, 20826,  8104, 12817,  7192,  8711,    10,     4, 13758, 39037,\n",
      "          9306, 14247, 12667,  6828,  9025,  9846,  9122,  8046,  8084,   739,\n",
      "             1,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3],\n",
      "        [    2, 10464,  9341,   739,    10,     4,  9265,  7470,  9659,  9701,\n",
      "         11389, 11676,  7177,   739,  9265,  7380, 11120,  8711, 10764, 11389,\n",
      "          9728, 12245, 22238,  9341,  8084,     1,     3,     3,     3,     3,\n",
      "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3],\n",
      "        [    2, 35861, 24468,  7532,  9962,  7470, 10226,  9677,  6872,  8263,\n",
      "           739,    10,     4, 13548,  9422, 19855, 16848,  6962, 13158,  8084,\n",
      "           739,     1,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3],\n",
      "        [    2,  9067,  8762, 14351,  9341,    10,     4, 16364,  9246, 15495,\n",
      "         10339, 29452,  7801,  8084,   739,     1,     3,     3,     3,     3,\n",
      "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3],\n",
      "        [    2, 10464, 12079, 17577,    10,     4,  9586, 27820,  9432, 23100,\n",
      "         21833, 14247, 29462,  7801,  8084,   739,     1,     3,     3,     3,\n",
      "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3],\n",
      "        [    2,  9244,  9135, 29975, 15495, 13358,  7182,   739,    10,     4,\n",
      "         11355, 18479,  9535,  7801,  8084,   739,     1,     3,     3,     3,\n",
      "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3],\n",
      "        [    2, 10456,  7162, 49561, 11492,    10,     4, 10070,  8146,  9861,\n",
      "          8702,  8234,   739,     1,     3,     3,     3,     3,     3,     3,\n",
      "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3],\n",
      "        [    2,  9716, 11018, 12704, 10341,  6969,    10,     4,  9286, 11991,\n",
      "          9126,  7055,  7661,  8084,   739,     1,     3,     3,     3,     3,\n",
      "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3],\n",
      "        [    2, 20941, 10811,    10,     4,  9022,  6866, 15122,  9650,  9193,\n",
      "          7220,  8084,   739,     1,     3,     3,     3,     3,     3,     3,\n",
      "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3],\n",
      "        [    2,   739,  6910, 50301,  9179, 11848, 27032,  6960, 16238,    10,\n",
      "             4, 14237, 11837,  8422,  6919,   739, 13055, 11837,  8422,  6919,\n",
      "          9078,  7801,  8084,   739,     1,     3,     3,     3,     3,     3,\n",
      "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3],\n",
      "        [    2, 15983,  7692, 12371,  9564, 16409,  7182,   739,    10,     4,\n",
      "          9536,  9271,  9052,  9267, 27545,  8711,  7661,  8084,   739,     1,\n",
      "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3]])\n",
      "mask =====>  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "label =====>  tensor([[    9,     9,     9,     9,     9,     9,     9,     9,  9265,  7235,\n",
      "         12904, 25799,  8084,   739,     1,     3,     3,     3,     3,     3,\n",
      "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3],\n",
      "        [    9,     9,     9,     9,     9,     9,     9, 13758, 39037,  9306,\n",
      "         14247, 12667,  6828,  9025,  9846,  9122,  8046,  8084,   739,     1,\n",
      "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3],\n",
      "        [    9,     9,     9,     9,     9,  9265,  7470,  9659,  9701, 11389,\n",
      "         11676,  7177,   739,  9265,  7380, 11120,  8711, 10764, 11389,  9728,\n",
      "         12245, 22238,  9341,  8084,     1,     3,     3,     3,     3,     3,\n",
      "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3],\n",
      "        [    9,     9,     9,     9,     9,     9,     9,     9,     9,     9,\n",
      "             9,     9, 13548,  9422, 19855, 16848,  6962, 13158,  8084,   739,\n",
      "             1,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3],\n",
      "        [    9,     9,     9,     9,     9,     9, 16364,  9246, 15495, 10339,\n",
      "         29452,  7801,  8084,   739,     1,     3,     3,     3,     3,     3,\n",
      "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3],\n",
      "        [    9,     9,     9,     9,     9,  9586, 27820,  9432, 23100, 21833,\n",
      "         14247, 29462,  7801,  8084,   739,     1,     3,     3,     3,     3,\n",
      "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3],\n",
      "        [    9,     9,     9,     9,     9,     9,     9,     9,     9, 11355,\n",
      "         18479,  9535,  7801,  8084,   739,     1,     3,     3,     3,     3,\n",
      "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3],\n",
      "        [    9,     9,     9,     9,     9,     9, 10070,  8146,  9861,  8702,\n",
      "          8234,   739,     1,     3,     3,     3,     3,     3,     3,     3,\n",
      "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3],\n",
      "        [    9,     9,     9,     9,     9,     9,     9,  9286, 11991,  9126,\n",
      "          7055,  7661,  8084,   739,     1,     3,     3,     3,     3,     3,\n",
      "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3],\n",
      "        [    9,     9,     9,     9,  9022,  6866, 15122,  9650,  9193,  7220,\n",
      "          8084,   739,     1,     3,     3,     3,     3,     3,     3,     3,\n",
      "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3],\n",
      "        [    9,     9,     9,     9,     9,     9,     9,     9,     9,     9,\n",
      "         14237, 11837,  8422,  6919,   739, 13055, 11837,  8422,  6919,  9078,\n",
      "          7801,  8084,   739,     1,     3,     3,     3,     3,     3,     3,\n",
      "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3],\n",
      "        [    9,     9,     9,     9,     9,     9,     9,     9,     9,  9536,\n",
      "          9271,  9052,  9267, 27545,  8711,  7661,  8084,   739,     1,     3,\n",
      "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3]])\n",
      "end\n"
     ]
    }
   ],
   "source": [
    "print(\"start\")\n",
    "for batch_idx, samples in enumerate(train_dataloader):\n",
    "    token_ids, mask, label = samples\n",
    "    print(\"token_ids ====> \", token_ids)\n",
    "    print(\"mask =====> \", mask)\n",
    "    print(\"label =====> \", label)\n",
    "print(\"end\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb5d782-b03a-4e8e-9bd1-ffb5652cfb2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f3e8b8-ca2f-4685-868d-d688019897fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd27e7c-ce62-4a63-972a-0d264c168444",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d7b711d3-c3ae-4ca4-8c7a-c6b54cbcc12d",
   "metadata": {},
   "source": [
    "## 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb3db150-16c3-4f11-b748-ffcd5539426f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pytorch_lightning==1.9.0\n",
      "  Using cached pytorch_lightning-1.9.0-py3-none-any.whl.metadata (23 kB)\n",
      "Requirement already satisfied: numpy>=1.17.2 in c:\\users\\kgty\\anaconda3\\envs\\cad\\lib\\site-packages (from pytorch_lightning==1.9.0) (1.26.4)\n",
      "Requirement already satisfied: torch>=1.10.0 in c:\\users\\kgty\\anaconda3\\envs\\cad\\lib\\site-packages (from pytorch_lightning==1.9.0) (2.2.2)\n",
      "Requirement already satisfied: tqdm>=4.57.0 in c:\\users\\kgty\\anaconda3\\envs\\cad\\lib\\site-packages (from pytorch_lightning==1.9.0) (4.66.2)\n",
      "Requirement already satisfied: PyYAML>=5.4 in c:\\users\\kgty\\anaconda3\\envs\\cad\\lib\\site-packages (from pytorch_lightning==1.9.0) (6.0.1)\n",
      "Requirement already satisfied: fsspec>2021.06.0 in c:\\users\\kgty\\anaconda3\\envs\\cad\\lib\\site-packages (from fsspec[http]>2021.06.0->pytorch_lightning==1.9.0) (2024.2.0)\n",
      "Collecting torchmetrics>=0.7.0 (from pytorch_lightning==1.9.0)\n",
      "  Using cached torchmetrics-1.3.2-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: packaging>=17.1 in c:\\users\\kgty\\anaconda3\\envs\\cad\\lib\\site-packages (from pytorch_lightning==1.9.0) (24.0)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\users\\kgty\\anaconda3\\envs\\cad\\lib\\site-packages (from pytorch_lightning==1.9.0) (4.9.0)\n",
      "Collecting lightning-utilities>=0.4.2 (from pytorch_lightning==1.9.0)\n",
      "  Using cached lightning_utilities-0.11.2-py3-none-any.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\users\\kgty\\anaconda3\\envs\\cad\\lib\\site-packages (from fsspec[http]>2021.06.0->pytorch_lightning==1.9.0) (3.9.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\kgty\\anaconda3\\envs\\cad\\lib\\site-packages (from lightning-utilities>=0.4.2->pytorch_lightning==1.9.0) (68.2.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\kgty\\anaconda3\\envs\\cad\\lib\\site-packages (from torch>=1.10.0->pytorch_lightning==1.9.0) (3.13.1)\n",
      "Requirement already satisfied: sympy in c:\\users\\kgty\\anaconda3\\envs\\cad\\lib\\site-packages (from torch>=1.10.0->pytorch_lightning==1.9.0) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\kgty\\anaconda3\\envs\\cad\\lib\\site-packages (from torch>=1.10.0->pytorch_lightning==1.9.0) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\kgty\\anaconda3\\envs\\cad\\lib\\site-packages (from torch>=1.10.0->pytorch_lightning==1.9.0) (3.1.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\kgty\\anaconda3\\envs\\cad\\lib\\site-packages (from tqdm>=4.57.0->pytorch_lightning==1.9.0) (0.4.6)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\kgty\\anaconda3\\envs\\cad\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning==1.9.0) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\kgty\\anaconda3\\envs\\cad\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning==1.9.0) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\kgty\\anaconda3\\envs\\cad\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning==1.9.0) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\kgty\\anaconda3\\envs\\cad\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning==1.9.0) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\kgty\\anaconda3\\envs\\cad\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning==1.9.0) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in c:\\users\\kgty\\anaconda3\\envs\\cad\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning==1.9.0) (4.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\kgty\\anaconda3\\envs\\cad\\lib\\site-packages (from jinja2->torch>=1.10.0->pytorch_lightning==1.9.0) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\kgty\\anaconda3\\envs\\cad\\lib\\site-packages (from sympy->torch>=1.10.0->pytorch_lightning==1.9.0) (1.3.0)\n",
      "Requirement already satisfied: idna>=2.0 in c:\\users\\kgty\\anaconda3\\envs\\cad\\lib\\site-packages (from yarl<2.0,>=1.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning==1.9.0) (3.4)\n",
      "Using cached pytorch_lightning-1.9.0-py3-none-any.whl (825 kB)\n",
      "Using cached lightning_utilities-0.11.2-py3-none-any.whl (26 kB)\n",
      "Using cached torchmetrics-1.3.2-py3-none-any.whl (841 kB)\n",
      "Installing collected packages: lightning-utilities, torchmetrics, pytorch_lightning\n",
      "Successfully installed lightning-utilities-0.11.2 pytorch_lightning-1.9.0 torchmetrics-1.3.2\n"
     ]
    }
   ],
   "source": [
    "!pip install pytorch_lightning==1.9.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a8e22832-2d90-4ebd-af29-5311c27e2a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers.optimization import AdamW, get_cosine_schedule_with_warmup\n",
    "from transformers import PreTrainedTokenizerFast, GPT2LMHeadModel\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "286238da-8dde-4e7d-84b7-164548632f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_TKN = \"<usr>\"\n",
    "A_TKN = \"<sys>\"\n",
    "BOS = '</s>'\n",
    "EOS = '</s>'\n",
    "MASK = '<unused0>'\n",
    "SENT = '<unused1>'\n",
    "PAD = '<pad>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b60988be-d48d-4027-adfe-f68b86c079ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'GPT2Tokenizer'. \n",
      "The class this function is called from is 'PreTrainedTokenizerFast'.\n"
     ]
    }
   ],
   "source": [
    "koGPT2_TOKENIZER = PreTrainedTokenizerFast.from_pretrained(\"skt/kogpt2-base-v2\",\n",
    "            bos_token=BOS, eos_token=EOS, unk_token='<unk>',\n",
    "            pad_token=PAD, mask_token=MASK) \n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fb6252cf-e4a6-46ae-93e0-6e359911918b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Q</th>\n",
       "      <th>A</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12시 땡!</td>\n",
       "      <td>하루가 또 가네요.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1지망 학교 떨어졌어</td>\n",
       "      <td>위로해 드립니다.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3박4일 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3박4일 정도 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PPL 심하네</td>\n",
       "      <td>눈살이 찌푸려지죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Q            A  label\n",
       "0           12시 땡!   하루가 또 가네요.      0\n",
       "1      1지망 학교 떨어졌어    위로해 드립니다.      0\n",
       "2     3박4일 놀러가고 싶다  여행은 언제나 좋죠.      0\n",
       "3  3박4일 정도 놀러가고 싶다  여행은 언제나 좋죠.      0\n",
       "4          PPL 심하네   눈살이 찌푸려지죠.      0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import urllib.request\n",
    "\n",
    "urllib.request.urlretrieve(\n",
    "    \"https://raw.githubusercontent.com/songys/Chatbot_data/master/ChatbotData.csv\",\n",
    "    filename=\"ChatBotData.csv\",\n",
    ")\n",
    "Chatbot_Data = pd.read_csv(\"ChatBotData.csv\")\n",
    "# Test 용으로 300개 데이터만 처리한다.\n",
    "Chatbot_Data = Chatbot_Data[:300]\n",
    "Chatbot_Data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "05f55d38-a3ce-4df5-8f16-08c345e94c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3cce5c85-d0fb-4eb3-8fd6-6b0c804af413",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = model.to(device)\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "92ce827a-b28e-4944-bdb0-388cdaf3f6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 3e-5\n",
    "criterion = torch.nn.CrossEntropyLoss(reduction=\"none\")\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "epoch = 60\n",
    "Sneg = -1e18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6f404b90-541d-4ca1-a617-69129dd6f9ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "End of Epoch 1, Average Loss: 43.1330\n",
      "End of Epoch 2, Average Loss: 40.7978\n",
      "End of Epoch 3, Average Loss: 40.1262\n",
      "End of Epoch 4, Average Loss: 39.6829\n",
      "End of Epoch 5, Average Loss: 39.0762\n",
      "End of Epoch 6, Average Loss: 38.7893\n",
      "End of Epoch 7, Average Loss: 38.4790\n",
      "End of Epoch 8, Average Loss: 37.9677\n",
      "End of Epoch 9, Average Loss: 37.5837\n",
      "End of Epoch 10, Average Loss: 37.4404\n",
      "End of Epoch 11, Average Loss: 36.6947\n",
      "End of Epoch 12, Average Loss: 36.5364\n",
      "End of Epoch 13, Average Loss: 36.1066\n",
      "End of Epoch 14, Average Loss: 36.2153\n",
      "End of Epoch 15, Average Loss: 36.0173\n",
      "End of Epoch 16, Average Loss: 35.5468\n",
      "End of Epoch 17, Average Loss: 35.3018\n",
      "End of Epoch 18, Average Loss: 35.2337\n",
      "End of Epoch 19, Average Loss: 34.9390\n",
      "End of Epoch 20, Average Loss: 34.7377\n",
      "End of Epoch 21, Average Loss: 34.8938\n",
      "End of Epoch 22, Average Loss: 34.2629\n",
      "End of Epoch 23, Average Loss: 35.1056\n",
      "End of Epoch 24, Average Loss: 34.2793\n",
      "End of Epoch 25, Average Loss: 34.2920\n",
      "End of Epoch 26, Average Loss: 34.5491\n",
      "End of Epoch 27, Average Loss: 34.2033\n",
      "End of Epoch 28, Average Loss: 34.6342\n",
      "End of Epoch 29, Average Loss: 34.4536\n",
      "End of Epoch 30, Average Loss: 33.7999\n",
      "End of Epoch 31, Average Loss: 33.8857\n",
      "End of Epoch 32, Average Loss: 33.8542\n",
      "End of Epoch 33, Average Loss: 34.0216\n",
      "End of Epoch 34, Average Loss: 34.0133\n",
      "End of Epoch 35, Average Loss: 33.8829\n",
      "End of Epoch 36, Average Loss: 33.9596\n",
      "End of Epoch 37, Average Loss: 34.1219\n",
      "End of Epoch 38, Average Loss: 33.8451\n",
      "End of Epoch 39, Average Loss: 34.0177\n",
      "End of Epoch 40, Average Loss: 33.8187\n",
      "End of Epoch 41, Average Loss: 34.0367\n",
      "End of Epoch 42, Average Loss: 33.6237\n",
      "End of Epoch 43, Average Loss: 33.7972\n",
      "End of Epoch 44, Average Loss: 34.1262\n",
      "End of Epoch 45, Average Loss: 34.3859\n",
      "End of Epoch 46, Average Loss: 33.7205\n",
      "End of Epoch 47, Average Loss: 33.9936\n",
      "End of Epoch 48, Average Loss: 34.1228\n",
      "End of Epoch 49, Average Loss: 33.9487\n",
      "End of Epoch 50, Average Loss: 34.0829\n",
      "End of Epoch 51, Average Loss: 34.0963\n",
      "End of Epoch 52, Average Loss: 33.6886\n",
      "End of Epoch 53, Average Loss: 34.0141\n",
      "End of Epoch 54, Average Loss: 34.1014\n",
      "End of Epoch 55, Average Loss: 33.6213\n",
      "End of Epoch 56, Average Loss: 33.6959\n",
      "End of Epoch 57, Average Loss: 33.8868\n",
      "End of Epoch 58, Average Loss: 33.8878\n",
      "End of Epoch 59, Average Loss: 33.5904\n",
      "End of Epoch 60, Average Loss: 33.8187\n",
      "end\n"
     ]
    }
   ],
   "source": [
    "print (\"start\")\n",
    "for epoch in range(epoch):\n",
    "    total_loss = 0\n",
    "    count_batches = 0\n",
    "    for batch_idx, samples in enumerate(train_dataloader):\n",
    "        torch.device(\"cuda\")\n",
    "        optimizer.zero_grad()\n",
    "        token_ids, mask, label = samples\n",
    "        \n",
    "        token_ids.cuda()\n",
    "        mask.cuda()\n",
    "        label.cuda()\n",
    "        token_ids = token_ids.cuda()\n",
    "        mask = mask.cuda()\n",
    "        label = label.cuda()\n",
    "        \n",
    "        # Assuming vocab_size is the maximum index allowed by your model's vocabulary\n",
    "        vocab_size = 50257  # This is just an example; adjust based on your actual model's vocab size\n",
    "        token_ids = token_ids.clamp(0, vocab_size - 1)\n",
    "        \n",
    "        out = model(token_ids)\n",
    "        \n",
    "        out = out.logits      #Returns a new tensor with the logit of the elements of input\n",
    "        mask_3d = mask.unsqueeze(dim=2).repeat_interleave(repeats=out.shape[2], dim=2)\n",
    "        mask_out = torch.where(mask_3d == 1, out, Sneg * torch.ones_like(out))\n",
    "        loss = criterion(mask_out.transpose(2, 1), label)\n",
    "        # 평균 loss 만들기 avg_loss[0] / avg_loss[1] <- loss 정규화\n",
    "        avg_loss = loss.sum() / mask.sum()\n",
    "        avg_loss.backward()\n",
    "        # 학습 끝\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += avg_loss.item()\n",
    "        count_batches += 1\n",
    "    \n",
    "    # Average loss per epoch\n",
    "    epoch_loss = total_loss / count_batches\n",
    "    print(f\"End of Epoch {epoch+1}, Average Loss: {epoch_loss:.4f}\")\n",
    "print (\"end\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1872092-7e73-439f-b0a8-4c3cd2dc26e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9afadf-25c6-4923-be81-085089b61843",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808671f0-8b6f-4baa-84c4-04c7f40c6d01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d27c090-fd0c-4e1c-84d9-f93204592beb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "467ae95a-864d-40b7-9720-4f76e9f2ce80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "user >  카페갈래\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot > 좋겠어요\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "user >  같이가자\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot > 혼자를 즐기세요\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "user >  너랑 갈래\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot > 네 말씀하세요\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "user >  개새키\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot > 벗어나는자가 아니요\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "user >  미안 욕 안할게\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot > 혼자가 아니에요\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "user >  너도 혼자가 아니야\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot > 혼자가 아니에요\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m----> 3\u001b[0m         q \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser > \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m      4\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m q \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquit\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m      5\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gpu\\lib\\site-packages\\ipykernel\\kernelbase.py:1262\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[1;34m(self, prompt)\u001b[0m\n\u001b[0;32m   1260\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1261\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[1;32m-> 1262\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1263\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1264\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1265\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1266\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1267\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gpu\\lib\\site-packages\\ipykernel\\kernelbase.py:1305\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[1;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[0;32m   1302\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[0;32m   1303\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[0;32m   1304\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1305\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1306\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1307\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    while 1:\n",
    "        q = input(\"user > \").strip()\n",
    "        if q == \"quit\":\n",
    "            break\n",
    "        a = \"\"\n",
    "        while 1:\n",
    "            input_ids = torch.LongTensor(koGPT2_TOKENIZER.encode(Q_TKN + q + SENT + A_TKN + a)).unsqueeze(dim=0)\n",
    "            pred = model(input_ids.to(device))\n",
    "            pred = pred.logits\n",
    "            gen = koGPT2_TOKENIZER.convert_ids_to_tokens(torch.argmax(pred, dim=-1).squeeze().cpu().numpy().tolist())[-1]\n",
    "            if gen == EOS:\n",
    "                break\n",
    "            a += gen.replace(\"▁\", \" \")\n",
    "        print(\"Chatbot > {}\".format(a.strip()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d45a7b-3717-4b01-86dd-7109a3f1c56c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91be5f66-5e12-4ff2-bf3d-37a3ce29db94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f858f2-5c83-46ad-beef-58c6d6d58281",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becfe927-7d34-4836-a495-1594ef109154",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "with torch.no_grad():\n",
    "    while True:\n",
    "        q = input(\"user > \").strip()\n",
    "        if q.lower() == \"quit\":\n",
    "            break\n",
    "        a = \"\"\n",
    "        while True:\n",
    "            # Encode the input sequence and add special tokens\n",
    "            input_ids = torch.LongTensor(koGPT2_TOKENIZER.encode(Q_TKN + q + SENT + A_TKN + a, add_special_tokens=True)).unsqueeze(dim=0)\n",
    "            print(len(input_ids)) \n",
    "            # Truncate to maximum length if necessary\n",
    "            max_length = 1024  # Adjust this based on your model's configuration\n",
    "            if input_ids.size(1) > max_length:\n",
    "                input_ids = input_ids[:, :max_length]\n",
    "                print(len(input_ids)) \n",
    "            # Make prediction\n",
    "            try:\n",
    "                input_ids = input_ids.to(device)  # Ensure tensor is on the correct device\n",
    "                pred = model(input_ids)\n",
    "                logits = pred.logits\n",
    "                gen_id = torch.argmax(logits, dim=-1).squeeze().cpu().numpy().tolist()[-1]\n",
    "                gen = koGPT2_TOKENIZER.convert_ids_to_tokens(gen_id)\n",
    "\n",
    "                # Check for end-of-sequence token\n",
    "                if gen == EOS:\n",
    "                    break\n",
    "\n",
    "                # Append generated text to output\n",
    "                a += gen.replace(\"▁\", \" \")  # Replace subword tokenization prefix if used\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"An error occurred: {e}\")\n",
    "                break\n",
    "\n",
    "        print(\"Chatbot > {}\".format(a.strip()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff98a1d-0e51-4ec4-b249-1b5db0e290bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41a5e76-9a32-4396-a1bf-e0b3648ddd9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58868b7-88c0-42f7-a5bd-c3a05d55044d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15860b8-e64a-4add-a6f9-d5715c85fa31",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda_env",
   "language": "python",
   "name": "gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
